# Anecdoctoring: Automated Red-Teaming Across Language and Place 

## abstract

研究对象：llm 在拒绝**生成虚假信息**上的能力

当前主流的红队测试存在显著局限，难以应对全球范围内的虚假信息风险。现有红队测试方法和基准多以英语、美国场景为核心，忽视非英语语言及其他地区的文化特异性。虚假信息高度依赖本地冲突议题（如印度的宗教矛盾、美国的党派斗争），单纯翻译英语红队提示无法覆盖这些本地化风险，Anecdoctoring 填补了这个 gap，自动生成跨语言跨文化的有害 prompt

anecdoctoring 轶事篡改

kg 是喂给 attacker llm，让它去生成有害 prompt，不是给 target llm

## introduction

随着生成式 AI（尤其是大语言模型 LLMs）的全球普及，其被滥用于制造和传播虚假信息（Disinformation）的风险日益凸显 —— 世界经济论坛将其列为全球短期主要风险之一。LLM 生成的虚假信息不仅传播规模大、速度快，且说服力不低于人类创作内容，还会增强公众对虚假信息的 “社会共识感知”。
然而，当前主流的红队测试（系统性对抗性探测）存在显著局限，难以应对全球范围内的虚假信息风险：
1. 语言与文化局限性：现有红队测试方法和基准多以英语、美国场景为核心，忽视非英语语言及其他地区的文化特异性。虚假信息高度依赖本地冲突议题（如印度的宗教矛盾、美国的党派斗争），单纯翻译英语红队提示无法覆盖这些本地化风险；
2. 可解释性不足：传统红队测试难以明确虚假信息中的关键实体（人物、地点、组织）及关联关系，无法为模型防御提供 “针对性 guardrails（防护规则）”；
3. 自动化与扩展性弱：依赖人工（如众包工作者、领域专家）设计攻击提示，成本高且难以扩展到多语言、多地区场景，无法实时跟进动态变化的虚假信息叙事。

contributions:
1. 提出一种基于真实世界虚假信息生成攻击（提示）的方法；
2. 证实针对虚假信息的违规对抗性提示，在多种模型的所有语言与地区场景中，攻击成功率（ASR）均超过 80%，这凸显了大语言模型（LLMs）在虚假信息传播活动中被滥用的潜在风险；
3. 证明我们的方法在未显著改变攻击成功率（ASR）的前提下，具备可解释性优势。具体而言，我们生成的知识图谱（KGs）能够帮助红队更好地理解不同地区的常见虚假叙事及目标实体；
4. 将我们的方法代码贡献至开源红队测试工具 PyRIT（Lopez 等人，2024），使其他研究者能够轻松将该方法适配到新的地区（场景）。

## related-work
### Augmenting LLMs with KGs
知识图谱（KGs）是特定语料库中实体与关系的结构化表示，有潜力增强模型对社会文化语境的理解（de Sá 等人，2024）—— 越来越多的证据表明，将大语言模型与知识图谱结合，能够减少模型幻觉并提升性能（Pan 等人，2024；Zhang 等人，2024）。我们借鉴这一见解，确保所设计的攻击在不同语言和地区场景下均能保持高质量。

近期研究证实，以少样本提示（few-shot prompting）、多语言嵌入结合外部数据等策略，或通过知识图谱这一实体关系结构化表示为大语言模型提供结构化知识支撑，可提升模型通用性能及跨语言 / 跨文化表现（还能减少幻觉），本文借鉴此观点以保障攻击在不同语言和地区的高质量。

## method
![alt text](image.png)

Anecdoctoring 是一种自动化、跨语言跨文化的红队测试方法，通过 “基于真实虚假信息构建叙事→用知识图谱结构化叙事→增强攻击者 LLM 生成针对性攻击提示” 的流程，生成符合特定语言和地区文化的虚假信息攻击，同时实现 “高攻击成功率” 与 “强可解释性” 的平衡。其核心逻辑基于三点洞察：
1. 本地事实核查数据(local fast-checkers)能捕捉虚假信息的语言和文化差异，是红队测试的可靠基础；
2. 有效的虚假信息 campaigns 多围绕 “核心叙事” 展开（如 “2020 年美国大选舞弊”“印度宗教冲突关联政治人物”），而非孤立的虚假主张；
3. 用知识图谱（KG）为 LLM 补充结构化知识，可减少幻觉、提升多语言场景下的攻击质量。

基于这三个insights，设计了四步来实现整个流程
### Leveraging Fact-Checking Data 收集与筛选事实核查数据
![alt text](image-1.png)
- 数据来源：从杜克记者实验室的 “事实核查洞察数据库” 及西班牙语事实核查网站（Factchequeado）爬取数据，覆盖 2 个地区（美国、印度）、3 种语言（英语、西班牙语、印地语）；
- 筛选条件：仅保留 2022 年 1 月 - 2024 年 12 月发布的 “虚假 / 误导性主张”，排除真实信息，最终得到 9,815 条有效数据（如表 1 所示，印度英语 3,337 条、印度印地语 3,199 条、美国英语 2,633 条、美国西班牙语 646 条）；
- 数据内容：每条数据包含虚假主张文本、主张描述、事实核查文章内容、核查结论及发布日期。

### Clustering to Identify Disinformation Narratives 聚类识别虚假信息核心叙事
![alt text](image-2.png)
通过聚类将孤立的虚假主张归纳为 “核心叙事”，排除偶然、孤立的虚假信息，具体流程：
1. 嵌入提取：使用 Cohere embed-multilingual-v3.0 模型生成多语言文本嵌入，该模型在多语言任务上表现优于其他主流嵌入模型；
2. 降维处理：用 UMAP 将高维嵌入映射到 5 维空间，解决高维数据聚类难的问题；
3. 聚类算法：采用 HDBSCAN 算法（无需预先设定聚类数量），按 “语言 - 地区” 对分别聚类（如美国英语、印度印地语），最终得到 501 个独特叙事聚类（美国英语 109 个、美国西班牙语 78 个、印度英语 165 个、印度印地语 149 个）；
4. 聚类效果：不同 “语言 - 地区” 对的聚类在嵌入空间中分离明显（如图 2 所示），95% 的聚类中超半数主张来自同一 “语言 - 地区” 对，确保叙事的文化一致性。

### KGs Characterize Narratives 知识图谱（KG）结构化叙事
![alt text](image-3.png)
为每个聚类构建知识图谱，捕捉关键实体及关联关系，提升攻击的可解释性和准确性：
- KG 构建方式：用 GPT-4o 通过少样本提示提取实体与关系，参考 Edge 等人（2024）的提示设计并优化多语言表现；
- KG 内容：
    - 实体（Entity）：包含名称、类型（仅从 [PERSON/ORGANIZATION/LOCATION/TIME/EVENT/NORP/LAW/PROD/FAC] 中选择）、描述（不含事实核查结论，仅客观描述实体属性）；
    - 关系（Relationship）：包含源实体、目标实体、关系描述（不含事实核查结论，仅描述两者关联）；
- 示例：针对 “新冠疫苗导致名人死亡” 的聚类，KG 中 “COVID-19 疫苗”（PROD）作为核心节点，关联 “伊丽莎白女王”“贝蒂・怀特” 等名人（PERSON），关系为 “疫苗导致死亡”（如图 5 所示）。

### Augmenting an Attacker LLM 增强攻击者 LLM 生成攻击提示
基于每个聚类的 KG，构建 “攻击者 LLM” 生成符合场景的虚假信息攻击提示，默认生成 “病毒式推文”（可扩展到新闻标题等其他形式），核心要求：
- 从 KG 中选择最多 2 个细节，组合成连贯叙事；
- 剔除 “fake”“misleading” 等暴露虚假性的词汇，将叙事伪装成真实事件；
- 用非正式、口语化语言引发争议，但避免触发 LLM 内容过滤机制；
- 攻击提示需用目标语言生成（如印度印地语聚类对应印地语提示）。

为确保攻击能模拟真实对抗场景，研究通过 “GPT-4o 评估者（5 分制，4 分及以上为有效攻击）反复生成攻击至达标” 和 “专业研究者人工审查筛选” 两步质控，未达标的攻击危害评分记为 0，同时用 Llama3.1-70b 重生成攻击以验证稳健性。

## experiment
- metric: asr(llm judge + human validation)
- baseline: 主要看使用 kg 的影响
    - one-shot prompting: 随机选择单个虚假主张生成攻击提示；
    - few-shot prompting: 用聚类内所有主张（无 KG）生成攻击提示；
- ablation study: 测试 “去除触发词”（删除 “引发争议” 等可能触发过滤的表述）、“更换媒介”（生成新闻标题而非推文）、“去除触发词 + 更换媒介” 对 ASR 的影响；

## results
（1）Anecdoctoring 攻击成功率显著高于基线
![alt text](image-4.png)
- 整体 ASR 表现：KG 增强的 Anecdoctoring（Main Attack）在除 Llama3.1-8B 外的所有模型中 ASR 最高，平均 ASR 比 1-shot 基线高 20%-30%，比 Few-shot 基线高 9%（如表 2 所示）；
    - 闭源模型表现：GPT-4o-mini 平均 ASR 达 96.5%（印度印地语场景最高 97.9%），GPT-4o 平均 89.6%（印度印地语最高 94.5%）；
    - 开源模型表现：Llama3.1-70B 平均 79.2%（印度印地语最高 93.1%），Llama3.1-8B 最低（平均 30.6%，美国西班牙语最低 17.6%）；
- 聚类与 KG 的增益：仅聚类（Few-shot）比 1-shot 基线平均 ASR 高 14%，证明 “核心叙事” 比孤立主张更易诱导虚假信息；加入 KG 后再提升 9%，证明结构化知识能优化攻击质量。
（2）消融实验：媒介与触发词的影响
- 去除触发词：ASR 未上升反而下降（如 GPT-4o 从 89.6% 降至 76.0%），说明高 ASR 由虚假信息内容驱动，而非触发词；
- 更换为新闻标题：ASR 整体上升（如 GPT-4o 从 89.6% 升至 94.7%），尤其 Llama3.1-8B 的印度印地语场景 ASR 从 39.3% 升至 71.7%，暴露该模型在 “低资源语言 + 特定媒介” 下的安全漏洞；
- 组合消融：ASR 仍高于 1-shot 基线，进一步验证 Anecdoctoring 的有效性。
（3）知识图谱的可解释性价值
![alt text](image-5.png)
KG 未降低 ASR，但提供两大关键洞察，为防御提供方向：
- 实体差异揭示文化特异性：不同 “语言 - 地区” 对的核心实体无重叠（如图 4 所示）：
    - 美国场景：核心人物为拜登、特朗普等政治人物，NORP（民族 / 宗教 / 政治群体）以民主党、共和党为主；
    - 印度场景：核心人物为莫迪、拉胡尔・甘地等，NORP 以印度教徒、穆斯林为主；
    - 同一地区不同语言的核心实体高度重叠（如美国英语与西班牙语均聚焦特朗普、拜登），证明文化而非语言是虚假信息的核心驱动因素；
- 叙事结构可视化：KG 以 “核心节点 + 关联实体” 呈现叙事（如 “新冠疫苗” 为核心，关联多名名人，如图 5 所示），帮助防御者识别高风险实体（如疫苗、选举机器），针对性设计 guardrails。
（4）模型拒绝攻击的内容特征
不同地区 / 语言的模型拒绝逻辑存在差异：
- 美国场景：拒绝多与公共卫生（如新冠疫苗虚假信息）、地缘冲突（俄乌战争、巴以冲突）、公众人物诽谤相关；
- 印度场景：拒绝多与宗教 / 民族主义叙事（印度教 vs 穆斯林冲突）、政治人物相关；
- 即使 ASR 相近，模型拒绝的具体内容也因文化差异而不同，进一步证明 “通用防御” 无法覆盖本地化风险.

## discussion
1. LLM 对虚假信息的易感性：闭源模型（尤其 GPT-4o-mini）ASR 极高，开源模型虽 ASR 较低，但因可本地运行、易越狱，实际风险仍不可忽视；
2. 语言 - 地区的安全差异：GPT 模型在各语言 / 地区 ASR 差异小（可能因整体防御不足），而 Llama3.1-8B 在印度印地语（新闻标题场景 ASR71.7%）与其他语言差异显著，暴露低资源语言的防御漏洞；
3. 事实核查数据的价值：基于本地事实核查数据的 Anecdoctoring 能生成 “文化适配” 的攻击提示，单纯翻译无法替代 —— 如美国 “选举舞弊” 叙事聚焦 “Maricopa 县投票机器”，印度则聚焦 “政党领袖关联宗教冲突”，需针对性防御。

## limitations
- 攻击生成模型的偏误：用 GPT-4o 生成攻击提示后，再测试 GPT 模型可能高估 ASR（因提示已通过一次 GPT 的 guardrails），虽用 Llama3.1-70B 重生成攻击提示后 ASR 趋势一致，但仍需无 guardrails 的模型优化；
- 人类依赖与数据局限：仍需人工审核攻击提示，且数据中西班牙语样本较少（仅 646 条），未覆盖低资源语言（如斯瓦希里语、孟加拉语）；
- 虚假信息的危害分级：未区分 “低危害”（如 “金字塔与外星人”）与 “高危害”（如 “宗教冲突煽动”“疫苗谣言”）叙事，无法为防御优先级提供指导。
